<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>无监督学习中的聚类算法</title>
      <link href="/2016/07/21/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
      <url>/2016/07/21/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>机器学习的三大部分一般包括模型、策略和算法，模型是指所选的机器学习算法应该具有某种特性或表达形式，策略是指如何从模型中选择出需要优化的目标函数，而算法则是使用什么优化算法来求解。就我个人的理解，模型和算法通过策略（优化的目标函数）间接关联，其本身有一定的独立性。<br>本篇文章将结合模型、策略和算法介绍无监督学习问题中常用的一些聚类算法，包括但不限于机器学习十大经典算法中的EM算法和K-Means算法。</p><h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><h3 id="模型说明"><a href="#模型说明" class="headerlink" title="模型说明"></a>模型说明</h3><p>考虑一个参数估计问题，现有$n$个训练样本$\lbrace y_1,y_2,\ldots,y_n \rbrace \in Y$，需要用多个参数$\theta$去拟合数据，那么这个问题的似然函数为</p><script type="math/tex; mode=display">\begin{equation}l(\theta)=\log P(Y\mid\theta)=\sum_{j=1}^n\log P(y_j\mid\theta)\end{equation}</script><p>此时模型的目标函数为</p><script type="math/tex; mode=display">\begin{equation}\widehat{\theta} = \arg \max_{\theta} l(\theta)\end{equation}</script><h3 id="算法推导"><a href="#算法推导" class="headerlink" title="算法推导"></a>算法推导</h3><p>由于$\theta$所表示的多个参数可能存在某种关系，导致上面的$\log$似然函数无法直接或者用梯度下降法求出最大值时的$\theta$值，因此需要引入隐变量$Z$来简化$l(\theta)$，使我们能够通过迭代的方法来求解出最优的$\theta$。<br>引入隐变量$Z$后，假设$Q(z)$是关于$z$的某种分布，则似然函数变为</p><script type="math/tex; mode=display">\begin{equation}\begin{split}l(\theta) &= \sum_{j=1}^{n} \log \sum_{i=1}^k P(y_{j},z_{i} \mid \theta) \\&= \sum_{j=1}^{n} \log \sum_{i=1}^k Q_{j}(z_{i})\frac{P(y_{j},z_{i}\mid \theta )}{Q_{j}(z_{i})} \\& \geq \sum_{j=1}^{n} \sum_{i=1}^k Q_{j}(z_{i})\log \frac{P(y_{j},z_{i}\mid \theta )}{Q_{j}(z_{i})}\end{split}\end{equation}</script><p>此处用到了Jensen不等式</p><blockquote><p>如果$f$是上凸函数，$X$是随机变量，那么$f(E[X]) \geq E[f(X)]$<br>特别地，如果$f$是严格上凸函数，那么$f(E[X])=E[f(X)]$当且仅当$X=E[X]$时成立，也就是说$X$是常量。</p></blockquote><p>公式中$\sum_{i=1}^k Q_j(z_i)\frac{P(y_j,z_i|\theta)}{Q_j(z_i)}$就是$[\frac{P(y_j,z_i|\theta)}{Q_j(z_i)}]$的期望值，而$\log$为上凸函数，因此若$\frac{P(y_j,z_i|\theta)}{Q_j(z_i)}=c$，则似然函数可以转化为</p><script type="math/tex; mode=display">\begin{equation}l(\theta) = \sum_{j=1}^{n} \sum_{i=1}^k Q_{j}(z_{i})\log \frac{P(y_{j},z_{i}\mid \theta )}{Q_{j}(z_{i})}\end{equation}</script><p>此时目标函数就能够用迭代的方法求出最优值，此时我们的问题就只剩下$Q_{j}(z_{i})$怎么求了，根据$\sum_{i=1}^k Q_{j}(z_{i})=1$，我们有</p><script type="math/tex; mode=display">\begin{equation}\begin{split}Q_j(z_i) &= \frac{P(y_j,z_i \mid \theta)}{c} \\&= \frac{P(y_j,z_i \mid \theta)}{\sum_{i=1}^k P(y_j,z_i \mid \theta)} \\&= \frac{P(y_j,z_i \mid \theta)}{P(y_j \mid \theta)} \\&= P(z_j \mid y_i,\theta)\end{split}\end{equation}</script><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><h4 id="形式1"><a href="#形式1" class="headerlink" title="形式1"></a>形式1</h4><blockquote><p>选取初始值$\theta_0$初始化$\theta$，$t=0$<br>Repeat{<br>E步：Expectation</p><script type="math/tex; mode=display">\begin{equation}Q_j^t(z_i) = P(z_j \mid y_i,\theta^t)\end{equation}</script><p>M步：Maximization</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\theta^{t+1} &= \arg \max_{\theta} \sum_{j=1}^{n} \sum_{i=1}^k Q_{j}^t(z_{i})\log \frac{P(y_{j},z_{i}\mid \theta )}{Q_{j}^t(z_{i})}\\t &= t+1\end{split}\end{equation}</script><p>}直到收敛</p></blockquote><h4 id="形式2"><a href="#形式2" class="headerlink" title="形式2"></a>形式2</h4><blockquote><p>选取初始值$\theta_0$初始化$\theta$，$t=0$<br>Repeat{<br>E步：Expectation</p><script type="math/tex; mode=display">\begin{equation}H(\theta,\theta^t) =\sum_{z=1}^k P(Z\mid Y,\theta ^{t})\log P(Y,Z\mid \theta )\end{equation}</script><p>M步：Maximization</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\theta^{t+1} &= \arg \max_{\theta} H(\theta,\theta^t) \\t &= t+1\end{split}\end{equation}</script><p>}直到收敛</p></blockquote><h3 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h3><p>只要有一些训练数据，再定义一个最大化函数，采用EM算法，利用计算机经过若干次迭代，就可以得到所需的模型。EM算法是自收敛的分类算法，既不需要事先设定类别也不需要数据见的两两比较合并等操作。缺点是当所要优化的函数不是凸函数时，EM算法容易给出局部最佳解，而不是最优解。</p><h2 id="EM算法的应用实例—高斯混合模型（GMM）的参数估计"><a href="#EM算法的应用实例—高斯混合模型（GMM）的参数估计" class="headerlink" title="EM算法的应用实例—高斯混合模型（GMM）的参数估计"></a>EM算法的应用实例—高斯混合模型（GMM）的参数估计</h2><h3 id="模型说明-1"><a href="#模型说明-1" class="headerlink" title="模型说明"></a>模型说明</h3><p>EM算法一般用于解决参数估计问题，无法独立解决实际问题。而GMM作为K个GSM的叠加，本质上属于一个参数估计问题。<br>对于单高斯模型，有</p><script type="math/tex; mode=display">\begin{equation}\phi \left ( y\mid \theta \right )= \frac{1}{\sqrt{2\pi }\sigma }\exp\left ( -\frac{\left ( y-\mu  \right )^{2}}{2\sigma^{2}} \right )\end{equation}</script><p>对于高斯混合模型，有</p><script type="math/tex; mode=display">\begin{equation}P(y\mid \theta )=\sum_{k=1}^{K}\alpha_{k}\phi (y\mid \theta_{k})\end{equation}</script><p>在这里，$\theta$包括3类参数$\alpha$，$\mu$和$\sigma$。</p><blockquote><p>对于一组数据$N_1,N_2,\ldots,N_k$且$N_1+N_2+\ldots+N_k=N$，如果我们事先知道数据的分类情况，那么它的三类参数就能表示为</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\alpha_{k}&=N_{k}/N \\\mu_{k}&=\frac{1}{N_{k}}\sum_{y\in S(k)}y \\\sigma_{k}&=\frac{1}{N_{k}}\sum_{y\in S(k)}(y-\mu_{k} )^{2}\end{split}\end{equation}</script></blockquote><p>而实际情况是我们只知道数据的观测值，需要根据观测值推测出数据的分类情况。<br>高斯混合模型给出了$P(y \mid \theta)$的假设，之后利用EM算法的思路，将参数估计问题转换成求解极大似然函数的极值点的问题，最后引入隐变量通过迭代方法求出最优值。</p><h3 id="算法推导-1"><a href="#算法推导-1" class="headerlink" title="算法推导"></a>算法推导</h3><p>对于一个高斯混合模型，假设它由$K$个单高斯模型组合而成，我们引入一个变量$\gamma_{jk}$，表示的是第$j$个观测数据来自第$k$类的概率，那么</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\gamma_{jk}&=P(z_{k}\mid y_{j},\theta ) \\& =\frac{P(z_{k},y_{j}\mid \theta )}{P(y_{j}\mid \theta )} \\& =\frac{P(z_{k},y_{j}\mid \theta )}{\sum_{k=1}^{K}P(z_{k},y_{j}\mid \theta )} \\& =\frac{P(y_{j}\mid z_{k},\theta )P(z_{k}\mid \theta )}{\sum_{k=1}^{K}P(y_{j}\mid z_{k},\theta )P(z_{k}\mid \theta)} \\& =\frac{\alpha_{k}\phi (y_{j}\mid \theta_{k})}{\sum_{k=1}^{K}\alpha_{k}\phi (y_{j}\mid \theta_{k})}\end{split}\end{equation}</script><p>这里对应于EM算法中的E步骤，$\gamma_{jk}$实际上就是EM算法原型中的$Q_j(z_i)$，只不过现在给出了它的实际意义。<br>因此在已知观测值的情况下，它的三类参数就可以表示为</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\alpha_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}}{N} \\\mu_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}y_{j}}{\sum_{j=1}^{N} \gamma_{jk}} \\\sigma^{2}_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}(y_{j}-\mu_{k})^{2}}{\sum_{j=1}^{N} \gamma_{jk}}\end{split}\end{equation}</script><p>这里对应于EM算法中的M步骤，混合高斯模型的三类参数实际上就是对极大似然函数求偏导的结果。</p><h3 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h3><h4 id="形式1-1"><a href="#形式1-1" class="headerlink" title="形式1"></a>形式1</h4><blockquote><p>选取初始值初始化$\theta$<br>重复{<br>E步：Expectation</p><script type="math/tex; mode=display">\begin{equation}\gamma_{jk}=\frac{\alpha_{k}\phi (y_{j}\mid \theta_{k})}{\sum_{k=1}^{K}\alpha_{k}\phi (y_{j}\mid \theta_{k})}\end{equation}</script><p>M步：Maximization</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\alpha_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}}{N} \\\mu_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}y_{j}}{\sum_{j=1}^{N} \gamma_{jk}} \\\sigma^{2}_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}(y_{j}-\mu_{k})^{2}}{\sum_{j=1}^{N} \gamma_{jk}}\end{split}\end{equation}</script><p>}直到收敛</p></blockquote><h4 id="形式2-1"><a href="#形式2-1" class="headerlink" title="形式2"></a>形式2</h4><blockquote><p>选取初始值初始化$\theta$<br>重复{<br>E步：Expectation</p><script type="math/tex; mode=display">\begin{equation}\begin{split} H(\theta ,\theta ^{t}) &= \sum_{j=1}^{N}\sum_{k=1}^{K}\gamma_{jk}\log\frac{\alpha_{k}\phi  (y_{j}\mid \theta_{k})}{\gamma_{jk}} \\& =\sum_{j=1}^{N}\sum_{k=1}^{K}\gamma_{jk}\log\frac{\frac{1}{\sqrt{2\pi }\sigma }\exp\left ( -\frac{\left ( y-\mu  \right )^{2}}{2\sigma^{2}} \right )}{\gamma_{jk}}\\& =\sum_{j=1}^{N}\sum_{k=1}^{K}\gamma_{jk}\left [\log \alpha_{k} +\log \left ( \frac{1}{\sqrt{2\pi }} \right ) -\log \sigma_{k}-\frac{(y-\mu )^{2}}{2\sigma_{k}^{2}}-\log\gamma_{jk} \right ]\end{split}\end{equation}</script><p>M步：Maximization<br>根据$H(\theta ,\theta ^{t})$和$\alpha_k \geq 0, \sum_{k=1}^K \alpha_k = 1$构造出$L(\theta)$<br>由</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\frac{ \partial L(\theta) }{\partial \alpha_{k}} &= 0 \\\frac{ \partial L(\theta) }{\partial \lambda} &= 0 \\\frac{\partial H(\theta, \theta^t)}{\partial\mu_k} &= 0 \\\frac{\partial H(\theta, \theta^t)}{\partial\sigma_k} &= 0\end{split}\end{equation}</script><p>得出</p><script type="math/tex; mode=display">\begin{equation}\begin{split}\alpha_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}}{N} \\\mu_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}y_{j}}{\sum_{j=1}^{N} \gamma_{jk}} \\\sigma^{2}_{k}&=\frac{\sum_{j=1}^{N} \gamma_{jk}(y_{j}-\mu_{k})^{2}}{\sum_{j=1}^{N} \gamma_{jk}}\end{split}\end{equation}</script><p>}直到收敛</p></blockquote><h2 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h2><h3 id="算法推导-2"><a href="#算法推导-2" class="headerlink" title="算法推导"></a>算法推导</h3><p>EM算法是利用相互迭代的思路求解最优值，同样利用迭代思想的还包括K-Means聚类算法。<br>K-Means算法定义了一个损失函数</p><script type="math/tex; mode=display">\begin{equation}J=\sum_{n=1}^N \sum_{k=1}^K r_{nk}\left|\left|x_n-\mu_k \right| \right|^2\end{equation}</script><p>此时模型的目标函数为</p><script type="math/tex; mode=display">\begin{equation}[\widehat r,\widehat \mu]=\arg \min J\end{equation}</script><p>与高斯混合模型中的目标函数相比，K-Mean算法的损失函数$r_{nk}\in \lbrace0,1\rbrace$，而似然函数的$\gamma_{jk}\in \left[0,1\right]$，当然还有其他比较明显的区别如K-Means是求和而GMM是对数求和，这与问题模型本身的性质有关。<br>K-Means算法中，我们的目标是找到合适的$r_{nk}$和$\mu_k$使损失函数最小，可以采用类似于EM算法的迭代思路来求解。首先，我们为$\mu_k$选择一些初始值。然后，在第一阶段，我们关于$r_{nk}$最小化$J$，保持$\mu_k$固定。在第二阶段，我们关于$\mu_k$最小化$J$，保持$r_{nk}$固定。不断重复这两个阶段优化直到收敛。我们会看到，更新$r_{nk}$和更新$\mu_k$的两个阶段分别对应于EM算法中的E（期望）步骤和M（最大化）步骤。<br>首先考虑确定$r_{nk}$。由于$J$是关于$r_{nk}$的一个线性函数，而不同的$n$（即样本）之间相互独立，因此我们可以对每个$n$分别进行最优化，只要$k$的值使$\left|\left|x_n-\mu_k \right| \right|^2$最小，我们就令$r_{nk}$等于1。换句话说，我们可以将数据点的聚类设置为最近的聚类中心。更形式化地，这可以表达为</p><script type="math/tex; mode=display">\begin{equation}\ r_{nk}=\begin{cases} 1，如果k=\arg min_j\left|\left|x_n-\mu_j \right| \right|^2\\ 0，其他情况\end{cases}\end{equation}</script><p>然后考虑$r_{nk}$固定时，关于$\mu_k$的最优化。目标函数$J$是$\mu_k$的一个二次函数，对其求偏导可以解出</p><script type="math/tex; mode=display">\begin{equation}\mu_k=\frac {\sum_{n} r_{nk}x_{n}}{\sum_n r_{nk}}\end{equation}</script><h3 id="算法步骤-2"><a href="#算法步骤-2" class="headerlink" title="算法步骤"></a>算法步骤</h3><blockquote><p>初始化：首先选择$K$个随机的点，称为聚类中心（cluster centroids）<br>Repeat{<br>（1）对于数据集中的每一个，按照距离与$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类<br>（2）计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置<br>}直到收敛</p></blockquote><h3 id="与高斯混合模型的异同点"><a href="#与高斯混合模型的异同点" class="headerlink" title="与高斯混合模型的异同点"></a>与高斯混合模型的异同点</h3><h4 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h4><p>（1）需要指定$K$值。<br>（2）需要指定初始值，例如K-Means的中心点，GMM的各个参数。<br>（3）都是含有EM算法思想。</p><h4 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h4><p>（1）优化目标函数不同，K-Means：最短距离，GMM：最大化$\log$似然估计。<br>（2）E步的指标不同，K-Means：点到聚类中心的距离（硬指标），GMM：求解每个观测数据属于每一类的概率（软指标）。</p><h3 id="算法优缺点-1"><a href="#算法优缺点-1" class="headerlink" title="算法优缺点"></a>算法优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><p>（1）是解决聚类问题的一种经典算法，简单、快速。<br>（2）对处理大数据集，该算法是相对可伸缩和高效率的。<br>（3）当类内数据密集，而类间数据区别明显时, 它的效果较好。</p><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>（1）在类的平均值被定义的情况下才能使用，这对于处理符号属性的数据不适用。<br>（2）必须事先给出$K$（要生成的类的数目），而且对初值敏感，对于不同的初始值，可能会导致不同结果。<br>（3）它对于“躁声”和孤立点数据是敏感的，少量的该类数据能够对平均值产生极大的影响。</p><h4 id="K-Means算法对于不同的初始值和类数，可能会导致不同结果，解决方法为"><a href="#K-Means算法对于不同的初始值和类数，可能会导致不同结果，解决方法为" class="headerlink" title="K-Means算法对于不同的初始值和类数，可能会导致不同结果，解决方法为"></a>K-Means算法对于不同的初始值和类数，可能会导致不同结果，解决方法为</h4><p>（1）多设置一些不同的初值，对比最后的运算结果，直到结果趋于稳定结束。<br>（2）绘出损失函数$J$关于类数$K$的曲线，利用”肘部法则“确定合适的$K$值。</p><h2 id="其他聚类算法"><a href="#其他聚类算法" class="headerlink" title="其他聚类算法"></a>其他聚类算法</h2><h3 id="K-Medoids聚类"><a href="#K-Medoids聚类" class="headerlink" title="K-Medoids聚类"></a>K-Medoids聚类</h3><p>K-Means算法的基础是将平方欧几里得距离作为数据点与代表向量之间不相似程度的度量。这不仅限制了能够处理的数据变量的类型（例如，它不能处理某些或全部变量表示类别标签的情形），而且使得聚类中心的确定对于异常点不具有鲁棒性。我们可以这样推广K-Means算法：引入两个向量$x$和$x’$之间的一个更加一般的不相似程度的度量$\nu(x,x’)$，然后最小化下面的损失函数</p><script type="math/tex; mode=display">\begin{equation}J=\sum_{n=1}^N \sum_{k=1}^K r_{nk}\nu(x_{n},\mu_{k})\end{equation}</script><h3 id="Spectral聚类"><a href="#Spectral聚类" class="headerlink" title="Spectral聚类"></a>Spectral聚类</h3><p>Spectral聚类的思想是将样本看作顶点，样本间的相似度看作带权的边，从而将聚类问题转为图分割问题：找到一种图分割的方法使得连接不同组的边的权重尽可能低，组内的边的权重尽可能高。<br>图分割问题可以定义为最小化以下目标函数</p><script type="math/tex; mode=display">\begin{equation}cut(A_{1},A_{2},\ldots,A{k})=\frac{1}{2}\sum_{i=1}^{k}W(A_{i},\overline {A_{i}})\end{equation}</script><p>其中$k$表示分成$k$个组，$A_{i}$表示第$i$个组，$W(A,B)$表示第$A$组与第$B$组之间的所有边的权重之和。<br>为保证每个类都有合理的大小，我们将目标函数调整为</p><script type="math/tex; mode=display">\begin{equation}\begin{split}RatioCut(A_{1},A_{2},\ldots,A{k})&=\frac{1}{2}\sum_{i=1}^{k}\frac{W(A_{i},\overline {A_{i}})}{\left|A_{i}\right|}\\&=\sum_{i=1}^{k}\frac{cut(A_{i},\overline {A_{i}})}{\left|A_{i}\right|}\end{split}\end{equation}</script><h3 id="Meanshift聚类"><a href="#Meanshift聚类" class="headerlink" title="Meanshift聚类"></a>Meanshift聚类</h3><p>MeanShift算法是一种非参数聚类技术，它不要求预先知道聚类的类别个数，对聚类的形状也没有限制。<br>在$d$维空间$R_{d}$中，给定$n$个数据点$x_{i}$，由核函数$K(x)$和窗口半径$h$得到的多元核密度估计函数为</p><script type="math/tex; mode=display">\begin{equation}f(x)=\frac{1}{nh^d} \sum_{i=1}^n K(\frac{x-x_{i}}{h})\end{equation}</script><p>其中</p><script type="math/tex; mode=display">\begin{equation}K(x)=c_{k,d}k(\left|\left| x\right|\right|^{2})\end{equation}</script><p>目标函数可以理解为</p><script type="math/tex; mode=display">\begin{equation}\widehat{x} = \arg \max_{x} f(x)\end{equation}</script><p>对$f(x)$求导</p><script type="math/tex; mode=display">\begin{equation}\nabla f(x)=\frac{2c_{k,d}}{nh^{d+2}}\sum_{i=1}^{n}(x_{i}-x)g(\left|\left| \frac{x-x_{i}}{h}\right|\right|^{2})=0\end{equation}</script><p>得出</p><script type="math/tex; mode=display">\begin{equation}x=\frac{\sum_{i=1}^{n}x_{i}g(\left|\left| \frac{x-x_{i}}{h}\right|\right|^{2})}  {\sum_{i=1}^{n}g(\left|\left| \frac{x-x_{i}}{h}\right|\right|^{2})}\end{equation}</script><p>令$g(x)=1$，发现$x=\frac{1}{n}\sum_{i=1}^{n}x_{i}$，那么</p><script type="math/tex; mode=display">\begin{equation}m_{h}(x)=\frac{\sum_{i=1}^{n}x_{i}g(\left|\left| \frac{x-x_{i}}{h}\right|\right|^{2})}  {\sum_{i=1}^{n}g(\left|\left| \frac{x-x_{i}}{h}\right|\right|^{2})}-x\end{equation}</script><p>反映的是样本均值的偏移量，即Meanshift向量。</p><blockquote><p>初始化：首先选择$K$个随机的点，称为聚类中心（cluster centroids）<br>Repeat{<br>（1）根据窗口半径$h$重新聚类以计算$m_{h}(x^{t})$<br>（2）平移窗口$x^{t+1}=x^{t}+m_{h}(x^{t})$<br>}直到收敛</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无监督学习 </tag>
            
            <tag> 聚类算法 </tag>
            
            <tag> EM </tag>
            
            <tag> GMM </tag>
            
            <tag> K-Means </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于实例的算法--KNN分类器</title>
      <link href="/2016/07/21/%E5%9F%BA%E4%BA%8E%E5%AE%9E%E4%BE%8B%E7%9A%84%E7%AE%97%E6%B3%95/"/>
      <url>/2016/07/21/%E5%9F%BA%E4%BA%8E%E5%AE%9E%E4%BE%8B%E7%9A%84%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="模型说明"><a href="#模型说明" class="headerlink" title="模型说明"></a>模型说明</h3><p>KNN分类算法的思路是：如果一个样本在特征空间中的$K$个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。<br>KNN分类算法包括三个基本要素：<br>（1）距离度量</p><blockquote><script type="math/tex; mode=display">\begin{equation}L_p(x_{i},x_{j})=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}\end{equation}</script></blockquote><p>（2）$K$值的选择</p><blockquote><p>$K$值越小，整体模型越复杂，容易发生过拟合<br>$K$值越大，整体模型越简单，近似误差会增大（误分类）</p></blockquote><p>（3）测试对象类别的判定</p><blockquote><p>多数表决</p><script type="math/tex; mode=display">\begin{equation}\widehat{\nu}=\arg \max_\nu \sum_{K} I(\nu=y_{i})\end{equation}</script><p>距离加权表决</p><script type="math/tex; mode=display">\begin{equation}\widehat{\nu}=\arg \max_\nu \sum_{K} \omega_{i}\times I(\nu=y_{i})\end{equation}</script></blockquote><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><blockquote><p>（1）给定测试对象，计算它与训练集中每个对象的距离<br>（2）圈定距离最近的$K$个对象，作为测试对象的近邻<br>（3）根据这k个近邻归属的主要类别，来对测试对象分类</p></blockquote><h3 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><p>（1）简单、有效。<br>（2）重新训练的代价较低（类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的）。<br>（3）计算时间和空间线性于训练集的规模（在一些场合不算太大）。<br>（4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。<br>（5）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。</p><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>（1）KNN算法是懒散学习方法（基本上不学习），一些积极学习的算法要快很多。<br>（2）类别评分不是规格化的（不像概率评分）。<br>（3）输出的可解释性不强，例如决策树的可解释性较强。<br>（4）该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。<br>（5）计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分类算法 </tag>
            
            <tag> KNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用VS2010和CMake编译OpenCV2.4.11</title>
      <link href="/2016/02/29/%E5%88%A9%E7%94%A8VS2010%E5%92%8CCMake%E7%BC%96%E8%AF%91OpenCV2.4.11/"/>
      <url>/2016/02/29/%E5%88%A9%E7%94%A8VS2010%E5%92%8CCMake%E7%BC%96%E8%AF%91OpenCV2.4.11/</url>
      
        <content type="html"><![CDATA[<h3 id="配置开发环境"><a href="#配置开发环境" class="headerlink" title="配置开发环境"></a>配置开发环境</h3><h4 id="下载与安装"><a href="#下载与安装" class="headerlink" title="下载与安装"></a>下载与安装</h4><ol><li><a href="http://opencv.org/downloads.html" target="_blank" rel="noopener">OpenCV2.4.11</a></li><li><a href="https://cmake.org/download/" target="_blank" rel="noopener">CMake</a></li></ol><h4 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h4><ol><li>在系统属性-环境变量-用户变量中新建变量，名为opencv，值为自己解压opencv路径下的build路径，如“D:\opencv\build”（路径名尽量不含空格，方便以后Matlab与OpenCV混合编程）</li><li>在系统属性-环境变量-系统变量中编辑Path变量，在末尾添加“ %opencv%\x86\vc10\bin;%opencv%\x64\vc10\bin”</li><li>编写OpenCV的工程属性表：在D:\opencv下新建文件opencv2411.props，编辑内容为：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;</span><br><span class="line">&lt;Project ToolsVersion=&quot;4.0&quot; xmlns=&quot;http://schemas.microsoft.com/developer/msbuild/2003&quot;&gt;</span><br><span class="line">  &lt;ImportGroup Label=&quot;PropertySheets&quot; /&gt;</span><br><span class="line">  &lt;PropertyGroup Label=&quot;UserMacros&quot; /&gt;</span><br><span class="line">  &lt;PropertyGroup&gt;</span><br><span class="line">    &lt;IncludePath&gt;$(OPENCV)\include;$(IncludePath)&lt;/IncludePath&gt;</span><br><span class="line">    &lt;LibraryPath Condition=&quot;&apos;$(Platform)&apos;==&apos;Win32&apos;&quot;&gt;$(OPENCV)\x86\vc10\lib;$(LibraryPath)&lt;/LibraryPath&gt;</span><br><span class="line">    &lt;LibraryPath Condition=&quot;&apos;$(Platform)&apos;==&apos;X64&apos;&quot;&gt;$(OPENCV)\x64\vc10\lib;$(LibraryPath)&lt;/LibraryPath&gt;</span><br><span class="line">  &lt;/PropertyGroup&gt;</span><br><span class="line">  &lt;ItemDefinitionGroup&gt;</span><br><span class="line">    &lt;Link Condition=&quot;&apos;$(Configuration)&apos;==&apos;Debug&apos;&quot;&gt;</span><br><span class="line">      &lt;AdditionalDependencies&gt;</span><br><span class="line">        opencv_calib3d2411d.lib;opencv_contrib2411d.lib;opencv_core2411d.lib;</span><br><span class="line">        opencv_features2d2411d.lib;opencv_flann2411d.lib;opencv_gpu2411d.lib;opencv_highgui2411d.lib;</span><br><span class="line">        opencv_imgproc2411d.lib;opencv_legacy2411d.lib;opencv_ml2411d.lib;opencv_nonfree2411d.lib;</span><br><span class="line">        opencv_objdetect2411d.lib;opencv_ocl2411d.lib;opencv_photo2411d.lib;opencv_stitching2411d.lib;</span><br><span class="line">        opencv_superres2411d.lib;opencv_ts2411d.lib;opencv_video2411d.lib;opencv_videostab2411d.lib;</span><br><span class="line">        %(AdditionalDependencies)</span><br><span class="line">      &lt;/AdditionalDependencies&gt;</span><br><span class="line">    &lt;/Link&gt;</span><br><span class="line">    &lt;Link Condition=&quot;&apos;$(Configuration)&apos;==&apos;Release&apos;&quot;&gt;</span><br><span class="line">      &lt;AdditionalDependencies&gt;</span><br><span class="line">        opencv_calib3d2411.lib;opencv_contrib2411.lib;opencv_core2411.lib;opencv_features2d2411.lib;</span><br><span class="line">        opencv_flann2411.lib;opencv_gpu2411.lib;opencv_highgui2411.lib;opencv_imgproc2411.lib;opencv_legacy2411.lib;</span><br><span class="line">        opencv_ml2411.lib;opencv_nonfree2411.lib;opencv_objdetect2411.lib;opencv_ocl2411.lib;opencv_photo2411.lib;</span><br><span class="line">        opencv_stitching2411.lib;opencv_superres2411.lib;opencv_ts2411.lib;opencv_video2411.lib;opencv_videostab2411.lib;</span><br><span class="line">        %(AdditionalDependencies)</span><br><span class="line">      &lt;/AdditionalDependencies&gt;</span><br><span class="line">    &lt;/Link&gt;</span><br><span class="line">  &lt;/ItemDefinitionGroup&gt;</span><br><span class="line">  &lt;ItemGroup /&gt;</span><br><span class="line">&lt;/Project&gt;</span><br></pre></td></tr></table></figure></li></ol><h3 id="利用CMake编译OpenCV2-4-11"><a href="#利用CMake编译OpenCV2-4-11" class="headerlink" title="利用CMake编译OpenCV2.4.11"></a>利用CMake编译OpenCV2.4.11</h3><ol><li>打开CMake软件，根据实际情况设置Where is the source code和Where to build为相应路径，如：<img src="./uploads/cmake.png" alt="Cmake编译OpenCV"></li><li>依次点击Configure和Generate</li></ol><h3 id="载入OpenCV工程和添加测试项目"><a href="#载入OpenCV工程和添加测试项目" class="headerlink" title="载入OpenCV工程和添加测试项目"></a>载入OpenCV工程和添加测试项目</h3><ol><li>点击OpenCV工程目录下的OpenCV.sln</li><li>右键解决方案资源管理器中的解决方案，选择添加-现有项目，在目录中选择测试项目的工程文件如cvut_test下的cvut_test.vcxproj，然后将cvut_test项目设为启动项目<img src="./uploads/cvut_test.png" alt="添加cvut_test项目"></li><li>右键属性管理器中的cvut_test，选择添加现有属性表，在目录中选择OpenCV的工程属性表如D:\opencv下的opencv2411.props</li><li>之后就可以方便地执行测试项目和查看其调用的OpenCV源码了</li></ol><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="http://blog.csdn.net/hnyzwtf/article/details/46403619" target="_blank" rel="noopener">win7 64位下VS2010和opencv 2.4.11的配置</a></li><li><a href="http://www.nmtree.net/2014/03/19/windows_build-opencv-with-cmake-and-vs2013.html" target="_blank" rel="noopener">Windows下利用CMake和VS2013编译OpenCV</a></li><li><a href="http://blog.sina.com.cn/s/blog_8b6c17eb0101l7zd.html" target="_blank" rel="noopener">opencv + cmake + vs2010 配置过程</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工程技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Visual Studio </tag>
            
            <tag> CMake </tag>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Github+Hexo+Next搭建个人博客</title>
      <link href="/2016/01/28/%E4%BD%BF%E7%94%A8Github-Hexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2016/01/28/%E4%BD%BF%E7%94%A8Github-Hexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h3 id="配置开发环境"><a href="#配置开发环境" class="headerlink" title="配置开发环境"></a>配置开发环境</h3><p>依次下载安装：</p><ul><li><a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">Node.js</a></li><li><a href="http://git-scm.com/download/" target="_blank" rel="noopener">Git</a></li></ul><h3 id="注册Github新建个人项目"><a href="#注册Github新建个人项目" class="headerlink" title="注册Github新建个人项目"></a>注册Github新建个人项目</h3><ol><li>新建一个Repository，将Repository name改为yourname.github.io，然后点击Create repository</li><li>在本地目标文件夹中右键选择Git Bash Here开始配置SSH-Key<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/.ssh</span><br><span class="line">$ ssh-keygen -t rsa -C <span class="string">"your e-mail"</span></span><br><span class="line">$ clip &lt; ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure></li><li>然后将其添加到Github的Settings-SSH keys中，之后就可以暂时关掉Github的网页了</li></ol><h3 id="通过Hexo框架部署网站"><a href="#通过Hexo框架部署网站" class="headerlink" title="通过Hexo框架部署网站"></a>通过Hexo框架部署网站</h3><ol><li>使用 git clone git@github.com:cyliuu/cyliuu.github.io.git 拷贝Repository到本地，然后在克隆后的本地文件夹内右键选择Git Bash Here开始初始化Hexo框架<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br><span class="line">$ npm install</span><br><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li><li>将_config.yml中倒数第二行的repo替换成你的</li><li>做完以上的步骤，你的博客其实已经搭建好了，你只需要把一些个人信息替换掉就行了</li></ol><h3 id="更换Next主题"><a href="#更换Next主题" class="headerlink" title="更换Next主题"></a>更换Next主题</h3><ol><li>由于我的模版里已经帮你下载好了，所以你的主要操作就是将theme/next/_config.yml中的个人信息替换成你的就行了</li><li>你可以像我一样引入多说评论接口，便于和网友们交流</li><li>接着再开启RSS订阅<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-generator-feed --save</span><br></pre></td></tr></table></figure></li><li>然后是生成本地网页<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></li><li>通过 <a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a> 本地接口访问网页，如果没有问题就可以发布了</li></ol><h3 id="使用Git上传和发布项目"><a href="#使用Git上传和发布项目" class="headerlink" title="使用Git上传和发布项目"></a>使用Git上传和发布项目</h3><ol><li>上传项目到缓存区<br>在本地文件夹内右键选择Git Bash Here，依次执行：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"# yourname.github.io"</span> &gt;&gt; README.md</span><br><span class="line">$ git init</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m <span class="string">"initial commit"</span></span><br><span class="line">$ git remote add origin https://github.com/yourname/yourname.github.io.git</span><br></pre></td></tr></table></figure></li><li>新建分支，将项目由缓存区上传到Github中<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git branch hexo</span><br><span class="line">$ git checkout hexo</span><br><span class="line">$ git push -u origin hexo</span><br></pre></td></tr></table></figure></li><li>部署网站<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></li><li>现在你可以通过 <a href="http://yourname.github.io/" target="_blank" rel="noopener">http://yourname.github.io/</a> 访问你的个人博客了</li><li>日常管理<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br><span class="line">$ git commit -m <span class="string">"..."</span></span><br><span class="line">$ git push origin hexo</span><br><span class="line">$ hexo generate --deploy</span><br></pre></td></tr></table></figure></li><li>本地数据丢失<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> git@github.com:yourname/yourname.github.io.git</span><br><span class="line">$ npm install -g hexo-cli</span><br><span class="line">$ npm install</span><br><span class="line">$ npm install hexo-deployer-git</span><br><span class="line">$ npm install hexo-generator-feed</span><br></pre></td></tr></table></figure></li></ol><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="http://crazymilk.github.io/2015/12/28/GitHub-Pages-Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">GitHub Pages + Hexo搭建博客</a></li><li><a href="http://moyatao.github.io/2015/11/04/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%EF%BC%9AGitHub-Hexo%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/#more" target="_blank" rel="noopener">我的第一篇博客：GitHub+Hexo搭建博客过程</a></li><li><a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">Next主题</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工程技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
