mathjax: true---
title: 基于实例的算法--KNN分类器
date: 2016-07-21 21:00:00
category: 机器学习
tags: [分类算法, KNN]
---
### 模型说明
KNN分类算法的思路是：如果一个样本在特征空间中的$K$个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。
KNN分类算法包括三个基本要素：
（1）距离度量
> $$\begin{equation}
> L_p(x_{i},x_{j})=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}
> \end{equation}$$

（2）$K$值的选择
> $K$值越小，整体模型越复杂，容易发生过拟合
> $K$值越大，整体模型越简单，近似误差会增大（误分类）

（3）测试对象类别的判定
> 多数表决
> $$\begin{equation}
> \widehat{\nu}=\arg \max_\nu \sum_{K} I(\nu=y_{i})
> \end{equation}$$
> 距离加权表决
> $$\begin{equation}
> \widehat{\nu}=\arg \max_\nu \sum_{K} \omega_{i}\times I(\nu=y_{i})
> \end{equation}$$

### 算法步骤
> （1）给定测试对象，计算它与训练集中每个对象的距离
> （2）圈定距离最近的$K$个对象，作为测试对象的近邻
> （3）根据这k个近邻归属的主要类别，来对测试对象分类

### 算法优缺点
#### 优点
（1）简单、有效。
（2）重新训练的代价较低（类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的）。
（3）计算时间和空间线性于训练集的规模（在一些场合不算太大）。
（4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。
（5）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。
#### 缺点
（1）KNN算法是懒散学习方法（基本上不学习），一些积极学习的算法要快很多。
（2）类别评分不是规格化的（不像概率评分）。
（3）输出的可解释性不强，例如决策树的可解释性较强。
（4）该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。
（5）计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。